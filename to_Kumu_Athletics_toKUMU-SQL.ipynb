{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "pd.set_option(\"max_rows\", None)\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the database\n",
    "\n",
    "server = ## \n",
    "database = ## \n",
    "username = ## \n",
    "password = ##\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "data = pd.read_sql_query('SELECT * FROM [panorama].[dbo].[athletics]',cnxn)\n",
    "\n",
    "data.rename(columns={\"Athletics or Varsity Sports - NAME\": \"Label\"},inplace = True)\n",
    "data.rename(columns={\"Country\": \"Type\"},inplace = True)\n",
    "\n",
    "data = data.replace(['nan'],float('NaN'))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append logos\n",
    "\n",
    "ids = data['Item ID'].tolist()\n",
    "logos = []\n",
    "\n",
    "for i in ids:\n",
    "    logos.append(\"https://scoutfyimagesbucket.s3-sa-east-1.amazonaws.com/athletics/\" + str(i) + \".png\")\n",
    "\n",
    "logos_data = pd.DataFrame({'Image': logos})\n",
    "data = pd.concat([data, logos_data], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing data to store/load\n",
    "\n",
    "\n",
    "name = data['Label'].tolist()\n",
    "\n",
    "ha_lst = data['HIERARCHICAL AFFILIATION'].tolist()\n",
    "hv_lst = data['HIERARCHICAL - Vinculated'].tolist()    \n",
    "hr_lst = data['HIERARCHICAL - Recognized'].tolist()\n",
    "\n",
    "#types = [\"Affiliation\",\"Vinculated\",\"Recognized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ha = {}      #to create a json file if necessary\n",
    "from_ = []    #names\n",
    "to_ = []      #affiliation\n",
    "type_ = []\n",
    "\n",
    "for i in range(len(name)):\n",
    "    try:\n",
    "        aux_lst = ha_lst[i].split(\"; \")\n",
    "        \n",
    "        dic_ha[name[i]] = aux_lst       \n",
    "        from_ += [name[i]] * len(aux_lst)\n",
    "        for k in aux_lst:\n",
    "            to_ += [k]\n",
    "            type_ += [\"Affiliation\"]\n",
    "            \n",
    "    #treating the error for \"nan\" values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#type_ = [\"Affiliation\"] * len(to_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_hv = {}      #to create a json file if necessary\n",
    "\n",
    "for i in range(len(name)):\n",
    "    try:\n",
    "        aux_lst = hv_lst[i].split(\"; \")\n",
    "        \n",
    "        dic_hv[name[i]] = aux_lst       \n",
    "        from_ += [name[i]] * len(aux_lst)\n",
    "        for k in aux_lst:\n",
    "            to_ += [k]\n",
    "            type_ += [\"Vinculated\"]\n",
    "            \n",
    "    #treating the error for \"nan\" values\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_hr = {}      #to create a json file if necessary\n",
    "\n",
    "for i in range(len(name)):\n",
    "    try:\n",
    "        aux_lst = hv_lst[i].split(\"; \")\n",
    "        \n",
    "        dic_hr[name[i]] = aux_lst       \n",
    "        from_ += [name[i]] * len(aux_lst)\n",
    "        for k in aux_lst:\n",
    "            to_ += [k]\n",
    "            type_ += [\"Recognized\"]\n",
    "            \n",
    "    #treating the error for \"nan\" values\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [data[\"Label\"], data[\"Athletics ACRONYM\"], data[\"Athletics NICKNAME\"],\n",
    "          data[\"Type\"],data[\"Facebook\"],data[\"Instagram\"],\n",
    "         data[\"Twitter\"],data[\"LinkedIn\"],data[\"Vimeo\"],data[\"Flickr\"],\n",
    "         data[\"Kind\"],data[\"State\"],data[\"City - STATE\"],\n",
    "         data[\"HIERARCHICAL AFFILIATION\"], data[\"Image\"]]\n",
    "\n",
    "result = pd.concat(frames,axis = 1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generating the dataFrame\n",
    "connections_data = pd.DataFrame({'From': from_,'To': to_,'Type': type_})\n",
    "connections_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing a new the excel file\n",
    "path = r\"updated-files/athletics_updated_toKumu-SQL.xlsx\"\n",
    "writer = pd.ExcelWriter(path, engine = 'xlsxwriter')\n",
    "\n",
    "#generating the two necessary sheets\n",
    "result.to_excel(writer, sheet_name = 'Elements',index=False)\n",
    "connections_data.to_excel(writer, sheet_name = 'Connections',index=False)\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
